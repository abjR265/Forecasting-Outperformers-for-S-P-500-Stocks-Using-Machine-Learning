{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: Various Methods\n",
    "\n",
    "> **Warning!** Please run `01_cleaning.ipynb` first if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functions.constants import BM_NAME, STARTDATE, ENDDATE, N_THRESHOLD_BPS, DATA_DIR, EVAL_START_DATE, TEST_START_DATE  # noqa: F401\n",
    "\n",
    "from functions.helper_fns import featurize_time_series,load_active_returns,evaluate_model_performance,get_X_and_y_values  # noqa: F401\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_returns = load_active_returns()\n",
    "active_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_period = \"1w\"\n",
    "chosen_feature_count = 12\n",
    "\n",
    "X_train, y_train, X_eval, y_eval, X_train_and_eval, y_train_and_eval, X_test, y_test, df_train, df_eval, df_train_and_eval, df_test = get_X_and_y_values(active_returns, chosen_period, chosen_feature_count)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_eval shape: \", X_eval.shape)\n",
    "print(\"y_eval shape: \", y_eval.shape)\n",
    "print(\"X_train_and_eval shape: \", X_train_and_eval.shape)\n",
    "print(\"y_train_and_eval shape: \", y_train_and_eval.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"df_train shape: \", df_train.shape)\n",
    "print(\"df_eval shape: \", df_eval.shape)\n",
    "print(\"df_train_and_eval shape: \", df_train_and_eval.shape)\n",
    "print(\"df_test shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(model_type, model_types_to_stack_if_stacking = [\"LogisticRegression\", \"RandomForest\",\"XGBoost\"]):\n",
    "    if model_type == \"LogisticRegression\":\n",
    "        model = LogisticRegression(class_weight=\"balanced\")\n",
    "    elif model_type == \"KNN\":\n",
    "        model = KNeighborsClassifier(n_neighbors=5) \n",
    "    elif model_type == \"RandomForest\":\n",
    "        model = RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, max_depth=5)\n",
    "    elif model_type == \"SVC\":\n",
    "        model = SVC(probability=True,max_iter=5000, class_weight=\"balanced\",kernel=\"rbf\")\n",
    "    elif model_type == \"XGBoost\":\n",
    "        model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    elif model_type == \"Stacking\":\n",
    "        estimators_models = []\n",
    "        print(\"!!!Stacking the following models!!!\")\n",
    "        for m in model_types_to_stack_if_stacking:\n",
    "            estimators_models.append((m,model_generator(m)))\n",
    "            print(m)\n",
    "        final_estimator = LogisticRegression(class_weight=\"balanced\")\n",
    "        stack = StackingClassifier(estimators=estimators_models,stack_method = 'predict_proba',n_jobs = -1,final_estimator = final_estimator)\n",
    "        model = stack\n",
    "    else:\n",
    "        raise ValueError(f\"Model Type {model_type} is not defined!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(MODEL_TYPE, NUM_FEATURES, PREDICTION_PERIOD):\n",
    "    model = model_generator(MODEL_TYPE)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_pred_proba = model.predict_proba(X_eval)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_proba = model.predict_proba(X_train)\n",
    "    print(\"===TRAINING SET===\")\n",
    "    evaluate_model_performance(y_train, y_train_pred, y_train_pred_proba,PREDICTION_PERIOD,NUM_FEATURES,plot_confusion_matrix=False)\n",
    "    print(\"===EVALUATION SET===\")\n",
    "    evaluate_model_performance(y_eval, y_pred, y_pred_proba,PREDICTION_PERIOD,NUM_FEATURES)\n",
    "    return model #this trained model holds the wisdom we need\n",
    "\n",
    "def write_eval_predictions_to_csv(model,model_name):\n",
    "    y_pred = model.predict(X_eval)\n",
    "    y_pred_proba = model.predict_proba(X_eval)\n",
    "    df_eval_with_predictions = df_eval.copy()\n",
    "    df_eval_with_predictions['outperform_{0}_predicted'.format(chosen_period)] = y_pred\n",
    "    df_eval_with_predictions['outperform_{0}_probability'.format(chosen_period)] = y_pred_proba[:, 1]\n",
    "    df_eval_with_predictions.to_csv(DATA_DIR + \"{0}_eval_predictions_{1}.csv\".format(BM_NAME, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = train_and_evaluate_model(\"LogisticRegression\", chosen_feature_count, chosen_period)\n",
    "write_eval_predictions_to_csv(logreg_model,\"LogisticRegression\")\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = train_and_evaluate_model(\"KNN\", chosen_feature_count, chosen_period)\n",
    "write_eval_predictions_to_csv(knn_model,\"KNN\")\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_model = train_and_evaluate_model(\"RandomForest\", chosen_feature_count, chosen_period)\n",
    "write_eval_predictions_to_csv(randomforest_model,\"RandomForest\")\n",
    "randomforest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = train_and_evaluate_model(\"XGBoost\", chosen_feature_count, chosen_period)\n",
    "write_eval_predictions_to_csv(xgboost_model,\"XGBoost\")\n",
    "xgboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = train_and_evaluate_model(\"SVC\", chosen_feature_count, chosen_period)\n",
    "write_eval_predictions_to_csv(svc_model,\"SVC\")\n",
    "svc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = train_and_evaluate_model(\"Stacking\",12,\"1w\")\n",
    "write_eval_predictions_to_csv(stacking_model,\"Stacking\")\n",
    "stacking_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best model based on evaluation dataset performance\n",
    "### As well as exporting predictions and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_model_name = \"Stacking\"\n",
    "optimum_model = model_generator(optimum_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on train and eval set\n",
    "optimum_model.fit(X_train_and_eval, y_train_and_eval)\n",
    "#predict on test set\n",
    "y_pred_test = optimum_model.predict(X_test)\n",
    "y_pred_proba_test = optimum_model.predict_proba(X_test)\n",
    "print(\"===TRAINING AND EVAL SET===\")\n",
    "y_train_and_eval_pred = optimum_model.predict(X_train_and_eval)\n",
    "y_train_and_eval_pred_proba = optimum_model.predict_proba(X_train_and_eval)\n",
    "evaluate_model_performance(y_train_and_eval, y_train_and_eval_pred, y_train_and_eval_pred_proba,chosen_period,chosen_feature_count,plot_confusion_matrix=False)\n",
    "print(\"===TEST SET===\")\n",
    "evaluate_model_performance(y_test, y_pred_test, y_pred_proba_test,chosen_period,chosen_feature_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the test set with the predictions and probability scores\n",
    "df_test_with_predictions = df_test.copy()\n",
    "df_test_with_predictions['outperform_{0}_predicted'.format(chosen_period)] = y_pred_test\n",
    "df_test_with_predictions['outperform_{0}_probability'.format(chosen_period)] = y_pred_proba_test[:,1]\n",
    "df_test_with_predictions = df_test_with_predictions.rename(columns={f\"ar_{chosen_period}_t\":f\"outperform_{chosen_period}_actual\"})\n",
    "df_test_with_predictions = df_test_with_predictions[['Date','Ticker',f'outperform_{chosen_period}_actual',f'outperform_{chosen_period}_predicted',f'outperform_{chosen_period}_probability']]\n",
    "#df_test_with_predictions\n",
    "#now do same for train and eval set\n",
    "df_train_and_eval_with_predictions = df_train_and_eval.copy()\n",
    "df_train_and_eval_with_predictions['outperform_{0}_predicted'.format(chosen_period)] = y_train_and_eval_pred\n",
    "df_train_and_eval_with_predictions['outperform_{0}_probability'.format(chosen_period)] = y_train_and_eval_pred_proba[:,1]\n",
    "df_train_and_eval_with_predictions = df_train_and_eval_with_predictions.rename(columns={f\"ar_{chosen_period}_t\":f\"outperform_{chosen_period}_actual\"})\n",
    "df_train_and_eval_with_predictions = df_train_and_eval_with_predictions[['Date','Ticker',f'outperform_{chosen_period}_actual',f'outperform_{chosen_period}_predicted',f'outperform_{chosen_period}_probability']]\n",
    "df_train_and_eval_with_predictions\n",
    "df_with_predictions = pd.concat([df_train_and_eval_with_predictions,df_test_with_predictions])\n",
    "df_with_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Brief Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the distribution of probabilities\n",
    "sns.histplot(df_with_predictions[f'outperform_{chosen_period}_probability'], kde=True)\n",
    "plt.title(f'Distribution of Probabilities for Outperforming {chosen_period}')\n",
    "#print mean and stdev on the plot at the top right corner\n",
    "mean_val = df_with_predictions[f'outperform_{chosen_period}_probability'].mean()\n",
    "stdev_val = df_with_predictions[f'outperform_{chosen_period}_probability'].std()\n",
    "plt.text(.75,.75, f\"Mean: {mean_val:.2f}\\nStdev: {stdev_val:.2f}\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv in DATA_DIR\n",
    "save_to_path = f\"{DATA_DIR}/{BM_NAME}_{chosen_period}_outperformance_predictions_{optimum_model_name}.csv\"\n",
    "df_with_predictions.to_csv(save_to_path, index=False)\n",
    "print(f\"Saved to {save_to_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
